{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import rcParams\n",
    "rcParams['font.size'] = 24\n",
    "rcParams['figure.figsize'] = (16, 8)\n",
    "from tqdm import tqdm \n",
    "\n",
    "import importlib \n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import Image\n",
    "import IPython\n",
    "\n",
    "from rllib.dataset.datatypes import Observation\n",
    "from rllib.util.utilities import get_entropy_and_log_p\n",
    "\n",
    "from rllib.util.training.agent_training import train_agent\n",
    "from rllib.environment import GymEnvironment\n",
    "from rllib.environment.mdps import EasyGridWorld\n",
    "from rllib.policy import TabularPolicy\n",
    "from rllib.value_function import TabularQFunction, TabularValueFunction\n",
    "from rllib.util.parameter_decay import ExponentialDecay\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_policy(q_function):\n",
    "    \"\"\"Extract a policy from the q_function.\"\"\"\n",
    "    policy = TabularPolicy(num_states=q_function.num_states,\n",
    "                           num_actions=q_function.num_actions)\n",
    "    for state in range(policy.num_states):\n",
    "        q_val = q_function(torch.tensor(state).long())\n",
    "        action = torch.argmax(q_val)\n",
    "\n",
    "        policy.set_value(state, action)\n",
    "\n",
    "    return policy\n",
    "\n",
    "def integrate_q(q_function, policy):\n",
    "    value_function = TabularValueFunction(num_states=q_function.num_states)\n",
    "    for state in range(policy.num_states):\n",
    "        state = torch.tensor(state).long()\n",
    "        pi = Categorical(logits=policy(state))\n",
    "        value = 0\n",
    "        for action in range(policy.num_actions):\n",
    "            value += pi.probs[action] * \\\n",
    "                q_function(state, torch.tensor(action).long())\n",
    "\n",
    "        value_function.set_value(state, value)\n",
    "\n",
    "    return value_function\n",
    "\n",
    "\n",
    "environment = EasyGridWorld()\n",
    "Image(\"images/grid_world.png\")\n",
    "\n",
    "# Plotters\n",
    "def plot_value_function(value_function, ax):\n",
    "    ax.imshow(value_function)\n",
    "    rows, cols = value_function.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            ax.text(j, i, f\"{value_function[i, j]:.1f}\",\n",
    "                    ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "def policy2str(policy):\n",
    "    left = u'\\u2190'\n",
    "    right = u'\\u2192'\n",
    "    up = u'\\u2191'\n",
    "    down = u'\\u2193'\n",
    "    policy_str = \"\"\n",
    "    if 0 == policy:\n",
    "        policy_str += down \n",
    "    if 1 == policy:\n",
    "        policy_str += up \n",
    "    if 2 == policy:\n",
    "        policy_str += right\n",
    "    if 3 == policy:\n",
    "        policy_str += left\n",
    "    return policy_str\n",
    "\n",
    "def plot_value_function(value_function, ax, title=None):\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(value_function)\n",
    "    rows, cols = value_function.shape\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            ax.text(row, col, f\"{value_function[col, row]:.1f}\", ha=\"center\", va=\"center\", color=\"w\", fontsize=24)\n",
    "\n",
    "def plot_policy(policy, ax, title):\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    rows, cols = policy.shape\n",
    "    ax.imshow(np.zeros((rows, cols)))\n",
    "    for row in range(environment.height):\n",
    "        for col in range(environment.width):\n",
    "            ax.text(col, row, policy2str(policy[row, col]), ha=\"center\", va=\"center\", color=\"r\", fontsize=24)\n",
    "\n",
    "\n",
    "def plot_value_and_policy(value_function, policy, title_value=None, title_policy=None):\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(20, 8))\n",
    "\n",
    "    plot_value_function(value_function, axes[0], title=title_value)\n",
    "    plot_policy(policy, axes[1], title=title_policy)\n",
    "    \n",
    "\n",
    "def plot_value_advantage_and_policy(value_function, advantage, policy, title_value=None, title_advantage=None, title_policy=None):\n",
    "    fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(20, 8))\n",
    "\n",
    "    plot_value_function(value_function, axes[0], title=title_value)\n",
    "    plot_value_function(advantage, axes[1], title=title_advantage)\n",
    "    plot_policy(policy, axes[2], title=title_policy)\n",
    "    \n",
    "    \n",
    "def init_value_function(num_states, terminal_states=None):\n",
    "    \"\"\"Initialize value function.\"\"\"\n",
    "    value_function = TabularValueFunction(num_states=num_states)\n",
    "    terminal_states = [] if terminal_states is None else terminal_states\n",
    "    for terminal_state in terminal_states:\n",
    "        value_function.set_value(terminal_state, 0)\n",
    "\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def build_mrp_matrices(environment, policy):\n",
    "    mrp_kernel = np.zeros((environment.num_states, 1, environment.num_states))\n",
    "    mrp_reward = np.zeros((environment.num_states, 1))\n",
    "\n",
    "    for state in range(environment.num_states):\n",
    "        state = torch.tensor(state).long()\n",
    "        policy_ = Categorical(logits=policy(state))\n",
    "\n",
    "        for a, p_action in enumerate(policy_.probs):\n",
    "            for transition in environment.transitions[(state.item(), a)]:\n",
    "                with torch.no_grad():\n",
    "                    p_ns = transition[\"probability\"]\n",
    "                    mrp_reward[state, 0] += p_action * p_ns * transition[\"reward\"]\n",
    "                    mrp_kernel[state, 0, transition[\"next_state\"]\n",
    "                               ] += p_action * p_ns\n",
    "\n",
    "    return mrp_kernel, mrp_reward\n",
    "\n",
    "\n",
    "def kernelreward2transitions(kernel, reward):\n",
    "    \"\"\"Transform a kernel and reward matrix into a transition dicitionary.\"\"\"\n",
    "    transitions = defaultdict(list)\n",
    "\n",
    "    num_states, num_actions = reward.shape\n",
    "\n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            for next_state in np.where(kernel[state, action])[0]:\n",
    "                transitions[(state, action)].append(\n",
    "                    {\n",
    "                        \"next_state\": next_state,\n",
    "                        \"probability\": kernel[state, action, next_state],\n",
    "                        \"reward\": reward[state, action],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return transitions\n",
    "\n",
    "def linear_system_policy_evaluation(environment, policy, gamma, value_function=None):\n",
    "    \"\"\"Evaluate a policy in an MDP solving the system bellman of equations.\n",
    "\n",
    "    V = r + gamma * P * V\n",
    "    V = (I - gamma * P)^-1 r\n",
    "    \"\"\"\n",
    "\n",
    "    if value_function is None:\n",
    "        value_function = init_value_function(environment.num_states)\n",
    "\n",
    "    kernel, reward = build_mrp_matrices(environment=environment, policy=policy)\n",
    "\n",
    "    A = torch.eye(environment.num_states) - gamma * kernel[:, 0, :]\n",
    "    # torch.testing.assert_allclose(A.inverse() @ A, torch.eye(model.num_states))\n",
    "    vals = A.inverse() @ reward[:, 0]\n",
    "    for state in range(environment.num_states):\n",
    "        value_function.set_value(state, vals[state].item())\n",
    "\n",
    "    return value_function\n",
    "\n",
    "def policy_iteration_step(environment, policy, value_function, gamma):\n",
    "    value_function = linear_system_policy_evaluation(environment, policy, gamma)\n",
    "    policy_stable = True\n",
    "    for state in range(environment.num_states):\n",
    "\n",
    "        value_ = torch.zeros(environment.num_actions)\n",
    "        for action in range(environment.num_actions):\n",
    "            value_estimate = 0\n",
    "            for transition in environment.transitions[(state, action)]:\n",
    "                next_state = torch.tensor(transition[\"next_state\"]).long()\n",
    "                reward = torch.tensor(transition[\"reward\"]).double()\n",
    "                value_estimate += transition[\"probability\"] * (\n",
    "                    reward + gamma * value_function(next_state).item()\n",
    "                )\n",
    "\n",
    "            value_[action] = value_estimate\n",
    "\n",
    "        state = torch.tensor(state).long()\n",
    "        old_policy = policy(state)\n",
    "        old_action = torch.argmax(old_policy)\n",
    "\n",
    "        action = torch.argmax(value_)\n",
    "        policy.set_value(state, action)\n",
    "\n",
    "        policy_stable &= (action == old_action).all().item()\n",
    "    \n",
    "    return value_function, policy_stable \n",
    "\n",
    "\n",
    "def policy_iteration(environment, gamma, max_iter=10, value_function=None, policy=None):\n",
    "    \"\"\"Implement Policy Iteration algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma: float.\n",
    "        discount factor.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.\n",
    "    MIT press.\n",
    "    Chapter 4.3\n",
    "\n",
    "    \"\"\"\n",
    "    if value_function is None:\n",
    "        value_function = init_value_function(environment.num_states)\n",
    "    if policy is None:\n",
    "        policy = TabularPolicy(num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "\n",
    "    for num_iter in range(max_iter):\n",
    "        # Evaluate the policy. \n",
    "        value_function, policy_stable = policy_iteration_step(environment, policy, value_function, gamma)\n",
    "        \n",
    "        if policy_stable:\n",
    "            break\n",
    "    return value_function, policy, num_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9123abfe4dc943e39ee0a074f9e011aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.9, continuous_update=False, description='gamma', max=0.99, step=0.01…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def reinforce(gamma, alpha, baseline):\n",
    "    output = ipywidgets.Output()\n",
    "    global policy \n",
    "    policy = TabularPolicy(num_states=environment.num_states,\n",
    "                           num_actions=environment.num_actions)\n",
    "    nn.init.ones_(policy.nn.head.weight)\n",
    "\n",
    "    episode_horizon = 20\n",
    "\n",
    "    def step(num_episodes):\n",
    "        global policy \n",
    "        for i_episode in range(num_episodes):\n",
    "            # The initial distribution plays a big role.\n",
    "            state = environment.reset()\n",
    "            trajectory = []\n",
    "            for i in range(episode_horizon):\n",
    "                pi = Categorical(logits=policy(torch.tensor(state).long()))\n",
    "                action = pi.sample().item()\n",
    "\n",
    "                next_state, reward, done, info = environment.step(action)\n",
    "                trajectory.append([state, action, reward, next_state])\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Calculate sum of discounted returns\n",
    "            cum_returns, rewards = 0, []\n",
    "            returns, grad_log_prob_action = [], []\n",
    "            for state, action, reward, next_state in trajectory[::-1]:\n",
    "                reward = reward[0]  # get the reward number.\n",
    "                cum_returns = reward + gamma * cum_returns  # why?\n",
    "                returns.insert(0, cum_returns)\n",
    "                grad_log_prob_action.insert(0, 1)  # In this case the gradient is very easy. \n",
    "\n",
    "            # Normalize returns for variance reduction.\n",
    "            returns = torch.tensor(returns)\n",
    "            if baseline:\n",
    "                returns = (returns - returns.mean()) / (returns.std() + 1e-12)\n",
    "            policy_grad = []\n",
    "            for grad_log_prob, ret in zip(grad_log_prob_action, returns):\n",
    "                policy_grad.append(-grad_log_prob * ret)\n",
    "\n",
    "            for (state, action, reward, next_state), grad in zip(trajectory, policy_grad):\n",
    "                policy.nn.head.weight.data[action, state] -= alpha * grad\n",
    "                \n",
    "        plot()\n",
    "\n",
    "    def plot():\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            value_function = linear_system_policy_evaluation(\n",
    "                environment, policy, gamma)\n",
    "            plot_value_and_policy(value_function.table.reshape(5, 5).detach().numpy(),\n",
    "                                  policy.table.argmax(0).reshape(5, 5).detach().numpy(), \n",
    "                                  title_value=\"Value Evaluation\",\n",
    "                                  title_policy=\"Policy\"\n",
    "                                 )\n",
    "            plt.show()\n",
    "\n",
    "    button = ipywidgets.Button(description=\"Step 10 Episodes\")\n",
    "    button.on_click(lambda b: step(num_episodes=10))\n",
    "    button2 = ipywidgets.Button(description=\"Step 100 Episodes\")\n",
    "    button2.on_click(lambda b: step(num_episodes=100))\n",
    "    display(output, button, button2)\n",
    "    plot()\n",
    "\n",
    "\n",
    "interact(\n",
    "    reinforce,\n",
    "    gamma=ipywidgets.FloatSlider(\n",
    "        value=0.9, min=0., max=0.99, step=1e-2, continuous_update=False),\n",
    "    alpha=ipywidgets.FloatSlider(\n",
    "        value=0.5, min=0., max=2.0, step=1e-2, continuous_update=False),\n",
    "    baseline=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Guide:\n",
    "\n",
    "#### some explanation\n",
    "This demo shows one of the basic reinforce policy gradient method REINFORCE. In this apporach we initialize the policy weight for the neural network which take the states as input and returns the probability of actions. So the initial policy is random. Secondly we use the policy to play N steps of the game — record action probabilities-from policy, reward-from environment, action — sampled by agent. Nextly we calculate the discounted reward for each step by backpropagation and calculate expected reward G. Finally we adjust weights of policy (back-propagate error in NN) to increase G.\n",
    "\n",
    "\n",
    "\n",
    "- gamma: the discount factor of the environment.\n",
    "- alpha: the learning rate in the Q learning\n",
    "- baseline: use the baselines to further reduce the variance.\n",
    "\n",
    "- step 10 Episode: Interacting the environment 1 time, generate one state, action, reward pair which has the length of horizon 20. \n",
    "- step 100 Episode: Interacting the environment 10 times, Note that the MDPs is learned after each iteration.\n",
    "\n",
    "- Value Evaluation: the value evaluated by the policy learned by the REINFORCE.\n",
    "- Policy: the optimal policy from the REINFORCE.\n",
    "\n",
    "\n",
    "#### play around\n",
    "- Run the REINFORCE with different alpha to see how fast it converges to the optimal policy or whether it stuck?\n",
    "- See the difference between REINFORCE and REINFORCE baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE w/ Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31182235653a46d482f63b3b251f7183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='env_name', options=('CartPole-v0', 'Acrobot-v1', 'MountainCar-v0')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run(env_name, agent_name):\n",
    "    if \"CartPole\" in env_name:\n",
    "        max_steps = 200\n",
    "    else:\n",
    "        max_steps = 1000 \n",
    "        \n",
    "    environment = GymEnvironment(env_name)\n",
    "    agent = getattr(\n",
    "        importlib.import_module(\"rllib.agent\"), \n",
    "        f\"{agent_name}Agent\"\n",
    "    ).default(environment, num_iter=1, num_rollouts=1)\n",
    "    \n",
    "    try:\n",
    "        train_agent(environment=environment, agent=agent, num_episodes=200, max_steps=max_steps, render=True, plot_flag=False)\n",
    "    except KeyboardInterrupt:\n",
    "        pass \n",
    "    environment.close()\n",
    "    \n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    plt.plot(agent.logger.get(\"train_return-0\"), linewidth=16)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return\");\n",
    "    \n",
    "interact(\n",
    "    run,\n",
    "    env_name = [\"CartPole-v0\", \"Acrobot-v1\", \"MountainCar-v0\"],\n",
    "    agent_name = [\"REINFORCE\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Guide:\n",
    "\n",
    "#### some explanation\n",
    "This demo shows running the REINFORCE algorithm using neural network as approxiamtion on different enviornment.\n",
    "- env_name: CartPole, Acrobot, MountainCar. Note that the Acrobot and MountainCar will take longer time for convergence.\n",
    "- agent_name: REINFORCE.  \n",
    "\n",
    "#### play around\n",
    "- Play with the REINFORCE on different environment\n",
    "- Compare the difference of the performance with Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5cd559cf154253894ecb8f5a550d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.9, continuous_update=False, description='gamma', max=0.99, step=0.01…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def actor_critic(gamma, alpha, eps):\n",
    "    output = ipywidgets.Output()\n",
    "    global policy, state, action \n",
    "    environment = EasyGridWorld()\n",
    "    _, pi_star, _ = policy_iteration(environment, gamma, max_iter=10)\n",
    "    pi_star = pi_star.table.argmax(0).reshape(5, 5).detach().numpy()\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    policy = TabularPolicy(num_states=environment.num_states,\n",
    "                           num_actions=environment.num_actions)\n",
    "    critic = TabularQFunction(num_states=environment.num_states,\n",
    "                              num_actions=environment.num_actions)\n",
    "    nn.init.ones_(policy.nn.head.weight)\n",
    "    \n",
    "    state = environment.reset()\n",
    "\n",
    "    if np.random.rand() < eps:\n",
    "        action = np.random.choice(environment.num_actions)\n",
    "    else:\n",
    "        action = Categorical(\n",
    "            logits=policy(torch.tensor(state).long())\n",
    "        ).sample().item()\n",
    "    def step(num_iter):\n",
    "        global state, action\n",
    "        for i in range(num_iter):\n",
    "            q_val = critic(torch.tensor(state).long(), torch.tensor(action).long())\n",
    "\n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "            \n",
    "            if np.random.rand() < eps:\n",
    "                next_action = np.random.choice(environment.num_actions)\n",
    "            else:\n",
    "                next_action = Categorical(\n",
    "                    logits=policy(torch.tensor(next_state).long())\n",
    "                ).sample().item()\n",
    "            \n",
    "            next_q = critic(torch.tensor(next_state).long(), torch.tensor(next_action).long())\n",
    "            \n",
    "            # update q function.\n",
    "            reward = torch.tensor(reward).double()\n",
    "            td = reward + gamma * next_q - q_val\n",
    "            critic.set_value(state, action, q_val + alpha * td)\n",
    "            \n",
    "            # update policy, here directly log_p is parameterized with a tabular funciton. \n",
    "            grad = -q_val.detach()\n",
    "            policy.nn.head.weight.data[action, state] -= alpha * grad[..., 0]\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "        plot()\n",
    "        \n",
    "    def plot():\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            def plot_(action):\n",
    "                fig, axes = plt.subplots(ncols=3, nrows=2, figsize=(20, 12))\n",
    "                value_function = integrate_q(critic, policy)\n",
    "                true_value_function = linear_system_policy_evaluation(environment, policy, gamma)\n",
    "\n",
    "                vf = value_function.table.reshape(5, 5).detach().numpy()\n",
    "                true_vf = true_value_function.table.reshape(5, 5).detach().numpy()\n",
    "                pi = policy.table.argmax(0).reshape(5, 5).detach().numpy()\n",
    "\n",
    "                if action == \"best\":\n",
    "                    q_star = critic.table.max(0)[0].reshape(5, 5).detach().numpy()\n",
    "                elif action == \"down\":\n",
    "                    q_star = critic.table[0].reshape(5, 5).detach().numpy()\n",
    "                elif action == \"up\":\n",
    "                    q_star = critic.table[1].reshape(5, 5).detach().numpy()\n",
    "                elif action == \"right\":\n",
    "                    q_star = critic.table[2].reshape(5, 5).detach().numpy()\n",
    "                elif action == \"left\":\n",
    "                    q_star = critic.table[3].reshape(5, 5).detach().numpy()\n",
    "\n",
    "\n",
    "                plot_value_function(vf, axes[0, 0], title=\"Estimated Value\")\n",
    "                plot_value_function(true_vf, axes[1, 0], title=\"True Value\")\n",
    "\n",
    "                plot_value_function(q_star, axes[0, 1], title=\"Estimated Critic\")\n",
    "                plot_value_function(q_star - vf, axes[1, 1], title=\"Estimated Advantage\")\n",
    "\n",
    "                plot_policy(pi, axes[0, 2], title=\"Current Policy\")\n",
    "                plot_policy(pi_star, axes[1, 2], title=\"Optimal Policy\")\n",
    "\n",
    "            interact(plot_,\n",
    "                     action=ipywidgets.ToggleButtons(\n",
    "                         options=[\"best\", \"down\", \"up\", \"left\", \"right\"],\n",
    "                         value='best',\n",
    "                         description='Critic Action:',\n",
    "                         style = {'description_width': 'initial'}\n",
    "                     ))\n",
    "\n",
    "    button = ipywidgets.Button(description=\"Step 100\")\n",
    "    button.on_click(lambda b: step(num_iter=100))\n",
    "    button2 = ipywidgets.Button(description=\"Step 1000\")\n",
    "    button2.on_click(lambda b: step(num_iter=1000))\n",
    "    display(output, button, button2)\n",
    "    plot()\n",
    "\n",
    "\n",
    "interact(\n",
    "    actor_critic,\n",
    "    gamma=ipywidgets.FloatSlider(\n",
    "        value=0.9, min=0., max=0.99, step=1e-2, continuous_update=False),\n",
    "    alpha=ipywidgets.FloatSlider(\n",
    "        value=0.6, min=0., max=1.0, step=1e-2, continuous_update=False),\n",
    "    eps=ipywidgets.FloatSlider(\n",
    "        value=0.25, min=0., max=0.5, step=1e-2, continuous_update=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Guide:\n",
    "\n",
    "#### some explanation\n",
    "This demo runs Actor Critic reinforcment learning approach:  In a simple term, Actor-Critic is a Temporal Difference(TD) version of Policy gradient. It has two networks: Actor and Critic. The actor decided which action should be taken and critic inform the actor how good was the action and how it should adjust. The learning of the actor is based on policy gradient approach. In comparison, critics evaluate the action produced by the actor by computing the value function.\n",
    "\n",
    "\n",
    "- gamma: the discount factor of the environment.\n",
    "- alpha: the learning rate in the Q learning\n",
    "- eps: the parameter trading the exploration and exploitation. With epislon probability will pick random action and 1-epislon pick the best action.\n",
    "- critic action:best, down, up, left, right: showthe estimate value of state-action pair.You have to choose the action first so given the action and states the critic will output the value function. \n",
    "- step 100: forward 100 steps and collect samples.\n",
    "- step 1000: forward 1000 steps and collect samples.\n",
    "\n",
    "- Estimated value: the estimated value function learned by interacting the world.\n",
    "- Estimated Critic: Given the action the value function from the critics. The best means the argmax of critic actions.\n",
    "- Current Policy: the optimal policy based on the learned by Actor-Critics\n",
    "- True Value: The ground Truth of the value function calculated by linear system policy evalution given the current polict. Used as comparison of estimated value.\n",
    "- Estimated Advantage: The advantage calculate the difference between value of state-action function and estimated value function.\n",
    "- Optimal Policy: The ground Truth of the optimal policy. Used as comparison of current policy\n",
    "\n",
    "\n",
    "#### play around\n",
    "- Do 100 step of default setup and see how the estimated value and critic changes\n",
    "- Change the critic action and see if it is reasonble for different action compared to the optimal policy action.\n",
    "- Tune the parameter and run the demo to see the convergence of estimated value and true value.\n",
    "- Check the change of estimated advantage, how does it change and when will it converge to 0?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic w/ Function Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976ceaf55dbd4e51aea0317cecb17ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='env_name', options=('CartPole-v0', 'Acrobot-v1'), value='CartPole-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rllib.util.losses import EntropyLoss\n",
    "def run(env_name, agent_name, eta):\n",
    "    if \"CartPole\" in env_name:\n",
    "        max_steps = 200\n",
    "    else:\n",
    "        max_steps = 1000 \n",
    "        \n",
    "    environment = GymEnvironment(env_name, seed=0)\n",
    "    agent = getattr(\n",
    "        importlib.import_module(\"rllib.agent\"), \n",
    "        f\"{agent_name}Agent\"\n",
    "    ).default(environment, num_iter=1, num_rollouts=1)\n",
    "    agent.algorithm.entropy_loss = EntropyLoss(\n",
    "        eta=eta * 0.01, regularization=True\n",
    "    )\n",
    "    try:\n",
    "        train_agent(environment=environment, agent=agent, num_episodes=200, max_steps=max_steps, render=True, plot_flag=False)\n",
    "    except KeyboardInterrupt:\n",
    "        pass \n",
    "    environment.close()\n",
    "    \n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    plt.plot(agent.logger.get(\"train_return-0\"), linewidth=16)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return\");\n",
    "    \n",
    "interact(\n",
    "    run,\n",
    "    env_name = [\"CartPole-v0\", \"Acrobot-v1\"],\n",
    "    agent_name = [\"GAAC\", \"A2C\", \"ActorCritic\"],\n",
    "    eta=ipywidgets.FloatSlider(value=0.2, min=0., max=1, step=0.1, continuous_update=False, description='entropy Reg:'),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Guide:\n",
    "\n",
    "#### some explanation\n",
    "This demo runs Actor Critic reinforcment learning method with function approximation using neural network.\n",
    "- env_name: CartPole, Acrobot, MountainCar. Note that the Acrobot and MountainCar will take longer time for convergence.\n",
    "- agent_name: [generalized advantage actor critic(GAAC)](https://arxiv.org/abs/1506.02438), [Advantage-Actor Critic(A2C)](https://proceedings.mlr.press/v48/mniha16.html), ActorCritic. More details can be found within the link: \n",
    "- entropy Reg: regularization parameter eta in entropy loss.In regularization mode, it returns a loss given by:\n",
    "\n",
    "$$ Loss = -\\eta * entropy. $$\n",
    "    \n",
    "In trust-region mode (regularization=False), it returns a loss given by:\n",
    "$$ Loss = \\eta * (entropy - target\\_entropy). $$\n",
    "\n",
    "#### play around\n",
    "Change the environment and agent name and see the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd1ce12c0f940cabca703750c23df71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='env_name', options=('CartPole-v0', 'Acrobot-v1', 'MountainCar-v0')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run(env_name, agent_name):\n",
    "    if \"CartPole\" in env_name:\n",
    "        max_steps = 200\n",
    "    else:\n",
    "        max_steps = 1000 \n",
    "        \n",
    "    environment = GymEnvironment(env_name)\n",
    "    agent = getattr(\n",
    "        importlib.import_module(\"rllib.agent\"), \n",
    "        f\"{agent_name}Agent\"\n",
    "    ).default(environment)\n",
    "    \n",
    "    try:\n",
    "        train_agent(environment=environment, agent=agent, num_episodes=200, max_steps=max_steps, render=True, plot_flag=False)\n",
    "    except KeyboardInterrupt:\n",
    "        pass \n",
    "    environment.close()\n",
    "    \n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    plt.plot(agent.logger.get(\"train_return-0\"), linewidth=16)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return\");\n",
    "    \n",
    "interact(\n",
    "    run,\n",
    "    env_name = [\"CartPole-v0\", \"Acrobot-v1\", \"MountainCar-v0\"],\n",
    "    agent_name = [\"TRPO\", \"PPO\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Guide:\n",
    "\n",
    "#### some explanation\n",
    "This demo runs constrained policy gradients method. TPRO iteratively optimizes a asurrogates objective within trust region. PPO is an effective heuristic variant.\n",
    "- env_name: CartPole, Acrobot, MountainCar. Note that the Acrobot and MountainCar will take longer time for convergence.\n",
    "- agent_name: [Trust region policy optimization(TPRO)](https://proceedings.mlr.press/v37/schulman15.html), [Proximal policy optimization algorithms(PPO)](https://arxiv.org/abs/1707.06347). More details can be found within the link: \n",
    "\n",
    "#### play around\n",
    "Change the environment and agent name and see the difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pathwise Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14479f858fb4945a571ce0f0588cb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='env_name', options=('MountainCarContinuous-v0', 'Pendulum-v1'), va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run(env_name, agent_name):\n",
    "    max_steps = 2500 \n",
    "        \n",
    "    environment = GymEnvironment(env_name)\n",
    "    agent = getattr(\n",
    "        importlib.import_module(\"rllib.agent\"), \n",
    "        f\"{agent_name}Agent\"\n",
    "    ).default(environment, exploration_episodes=10)\n",
    "    agent.params[\"exploration_noise\"] = ExponentialDecay(start=1.0, end=0.2, decay=1e5)\n",
    "    agent.policy.dist_params.update(policy_noise = agent.params[\"exploration_noise\"])\n",
    "    try:\n",
    "        train_agent(environment=environment, agent=agent, num_episodes=25, max_steps=max_steps, render=True, plot_flag=False)\n",
    "    except KeyboardInterrupt:\n",
    "        pass \n",
    "    environment.close()\n",
    "    \n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    plt.plot(agent.logger.get(\"train_return-0\"), linewidth=16)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return\");\n",
    "    \n",
    "interact(\n",
    "    run,\n",
    "    env_name = [ \"MountainCarContinuous-v0\", \"Pendulum-v1\"],\n",
    "    agent_name = [\"TD3\", \"SAC\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Guide:\n",
    "\n",
    "#### some explanation\n",
    "This demo runs off policy polict gradient algorithms. TD3 is extension of DDPG to avoid maximization bias. SAC method is variant of TD3 for entropy regualized MDPs. More details could be checked in the following link.\n",
    "\n",
    "\n",
    "- env_name: MountainCarContinuous-v0, Pendulum-v0, HalfCheetah-v2.\n",
    "- agent_name: [Trust region policy optimization(TPRO)](https://proceedings.mlr.press/v37/schulman15.html), [Proximal policy optimization algorithms(PPO)](https://arxiv.org/abs/1707.06347). More details can be found within the link: \n",
    "\n",
    "#### play around\n",
    "Change the environment and agent name and see the difference.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

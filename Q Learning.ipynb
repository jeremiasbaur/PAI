{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import rcParams\n",
    "rcParams['font.size'] = 24\n",
    "rcParams['figure.figsize'] = (16, 8)\n",
    "from tqdm import tqdm \n",
    "\n",
    "import importlib \n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import Image\n",
    "import IPython\n",
    "\n",
    "from rllib.dataset.datatypes import Observation\n",
    "from rllib.util.utilities import get_entropy_and_log_p\n",
    "\n",
    "from rllib.util.training.agent_training import train_agent\n",
    "from rllib.environment import GymEnvironment\n",
    "from rllib.environment.mdps import EasyGridWorld\n",
    "from rllib.policy import TabularPolicy\n",
    "from rllib.value_function import TabularQFunction, TabularValueFunction\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_policy(q_function):\n",
    "    \"\"\"Extract a policy from the q_function.\"\"\"\n",
    "    policy = TabularPolicy(num_states=q_function.num_states,\n",
    "                           num_actions=q_function.num_actions)\n",
    "    for state in range(policy.num_states):\n",
    "        q_val = q_function(torch.tensor(state).long())\n",
    "        action = torch.argmax(q_val)\n",
    "\n",
    "        policy.set_value(state, action)\n",
    "\n",
    "    return policy\n",
    "\n",
    "def integrate_q(q_function, policy):\n",
    "    value_function = TabularValueFunction(num_states=q_function.num_states)\n",
    "    for state in range(policy.num_states):\n",
    "        state = torch.tensor(state).long()\n",
    "        pi = Categorical(logits=policy(state))\n",
    "        value = 0\n",
    "        for action in range(policy.num_actions):\n",
    "            value += pi.probs[action] * \\\n",
    "                q_function(state, torch.tensor(action).long())\n",
    "\n",
    "        value_function.set_value(state, value)\n",
    "\n",
    "    return value_function\n",
    "\n",
    "\n",
    "environment = EasyGridWorld()\n",
    "Image(\"images/grid_world.png\")\n",
    "\n",
    "# Plotters\n",
    "def plot_value_function(value_function, ax):\n",
    "    ax.imshow(value_function, vmin=-1, vmax=25)\n",
    "    rows, cols = value_function.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            ax.text(j, i, f\"{value_function[i, j]:.1f}\",\n",
    "                    ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "def policy2str(policy):\n",
    "    left = u'\\u2190'\n",
    "    right = u'\\u2192'\n",
    "    up = u'\\u2191'\n",
    "    down = u'\\u2193'\n",
    "    policy_str = \"\"\n",
    "    if 0 == policy:\n",
    "        policy_str += down \n",
    "    if 1 == policy:\n",
    "        policy_str += up \n",
    "    if 2 == policy:\n",
    "        policy_str += right\n",
    "    if 3 == policy:\n",
    "        policy_str += left\n",
    "    return policy_str\n",
    "\n",
    "def plot_value_function(value_function, ax):\n",
    "    ax.imshow(value_function, vmin=-4, vmax=30)\n",
    "    rows, cols = value_function.shape\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            ax.text(row, col, f\"{value_function[col, row]:.1f}\", ha=\"center\", va=\"center\", color=\"w\", fontsize=24)\n",
    "\n",
    "def plot_policy(policy, ax):\n",
    "    rows, cols = policy.shape\n",
    "    ax.imshow(np.zeros((rows, cols)))\n",
    "    for row in range(environment.height):\n",
    "        for col in range(environment.width):\n",
    "            ax.text(col, row, policy2str(policy[row, col]), ha=\"center\", va=\"center\", color=\"r\", fontsize=24)\n",
    "\n",
    "\n",
    "def plot_value_and_policy(value_function, policy):\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(20, 8))\n",
    "\n",
    "    plot_value_function(value_function, axes[0])\n",
    "    plot_policy(policy, axes[1])\n",
    "    \n",
    "class EpsGreedyPolicy(object):\n",
    "    def __init__(self, policy, epsilon):\n",
    "        self.policy = policy\n",
    "        self.epsilon = epsilon \n",
    "    \n",
    "    def __call__(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            torch.tensor(state).long()\n",
    "        dist = Categorical(logits=self.policy(state))\n",
    "        probs = dist.probs\n",
    "        mixture = probs * (1 - self.epsilon) + self.epsilon / self.policy.num_actions\n",
    "        return Categorical(probs=mixture).logits\n",
    "\n",
    "def init_value_function(num_states, terminal_states=None):\n",
    "    \"\"\"Initialize value function.\"\"\"\n",
    "    value_function = TabularValueFunction(num_states=num_states)\n",
    "    terminal_states = [] if terminal_states is None else terminal_states\n",
    "    for terminal_state in terminal_states:\n",
    "        value_function.set_value(terminal_state, 0)\n",
    "\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def build_mrp_matrices(environment, policy):\n",
    "    mrp_kernel = np.zeros((environment.num_states, 1, environment.num_states))\n",
    "    mrp_reward = np.zeros((environment.num_states, 1))\n",
    "\n",
    "    for state in range(environment.num_states):\n",
    "        state = torch.tensor(state).long()\n",
    "        policy_ = Categorical(logits=policy(state))\n",
    "\n",
    "        for a, p_action in enumerate(policy_.probs):\n",
    "            for transition in environment.transitions[(state.item(), a)]:\n",
    "                with torch.no_grad():\n",
    "                    p_ns = transition[\"probability\"]\n",
    "                    mrp_reward[state, 0] += p_action * p_ns * transition[\"reward\"]\n",
    "                    mrp_kernel[state, 0, transition[\"next_state\"]\n",
    "                               ] += p_action * p_ns\n",
    "\n",
    "    return mrp_kernel, mrp_reward\n",
    "\n",
    "def linear_system_policy_evaluation(environment, policy, gamma, value_function=None):\n",
    "    \"\"\"Evaluate a policy in an MDP solving the system bellman of equations.\n",
    "\n",
    "    V = r + gamma * P * V\n",
    "    V = (I - gamma * P)^-1 r\n",
    "    \"\"\"\n",
    "\n",
    "    if value_function is None:\n",
    "        value_function = init_value_function(environment.num_states)\n",
    "\n",
    "    kernel, reward = build_mrp_matrices(environment=environment, policy=policy)\n",
    "\n",
    "    A = torch.eye(environment.num_states) - gamma * kernel[:, 0, :]\n",
    "    # torch.testing.assert_allclose(A.inverse() @ A, torch.eye(model.num_states))\n",
    "    vals = A.inverse() @ reward[:, 0]\n",
    "    for state in range(environment.num_states):\n",
    "        value_function.set_value(state, vals[state].item())\n",
    "\n",
    "    return value_function\n",
    "\n",
    "def plot_all(estimated_value, policy, exploration_value_function, testing_value_function):\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(20, 6))\n",
    "    plot_value_function(estimated_value, axes[0])\n",
    "    axes[0].set_title('Estimated Value')\n",
    "    plot_policy(policy, axes[1])\n",
    "    axes[1].set_title('Policy')\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(20, 6))\n",
    "    plot_value_function(exploration_value_function, axes[0])\n",
    "    axes[0].set_title('Exploration Value')\n",
    "    plot_value_function(testing_value_function, axes[1])\n",
    "    axes[1].set_title('Testing Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfe0178ef934be8b5e7013402fef1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.9, continuous_update=False, description='gamma', max=0.99, step=0.01â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def q_learning(gamma=0.9, alpha=0.5, eps=0., optimistic_init=False):\n",
    "    output = ipywidgets.Output()\n",
    "    \n",
    "    global state \n",
    "    q_function = TabularQFunction(\n",
    "        num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "    nn.init.ones_(q_function.nn.head.weight)\n",
    "\n",
    "    if optimistic_init:\n",
    "        q_function.nn.head.weight.data = 10 / \\\n",
    "            (1 - gamma) * q_function.nn.head.weight.data  # Initialization\n",
    "\n",
    "    state = environment.reset()\n",
    "    state = 0 \n",
    "    environment.state = state \n",
    "\n",
    "    def step(num_iter):\n",
    "        global state\n",
    "\n",
    "        for i in range(num_iter):\n",
    "            if np.random.rand() < eps:\n",
    "                action = np.random.choice(environment.num_actions)\n",
    "            else:\n",
    "                action = torch.argmax(q_function(\n",
    "                    torch.tensor(state).long())).item()\n",
    "\n",
    "            q_val = q_function(torch.tensor(state).long(),\n",
    "                               torch.tensor(action).long())\n",
    "\n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "\n",
    "            next_q = torch.max(q_function(torch.tensor(next_state).long()))\n",
    "            reward = torch.tensor(reward).double()\n",
    "            td = reward + gamma * next_q - q_val\n",
    "\n",
    "            q_function.set_value(state, action, q_val + alpha * td)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        plot()\n",
    "\n",
    "    def plot():\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            policy = extract_policy(q_function)\n",
    "            value_function = integrate_q(q_function, policy)\n",
    "            exploration_vf = linear_system_policy_evaluation(environment, EpsGreedyPolicy(policy, eps), gamma)\n",
    "            testing_vf = linear_system_policy_evaluation(environment, policy, gamma)\n",
    "\n",
    "            plot_all(\n",
    "                value_function.table.reshape(5, 5).detach().numpy(),\n",
    "                policy.table.argmax(0).reshape(5, 5).detach().numpy(),\n",
    "                exploration_vf.table.reshape(5, 5).detach().numpy(), \n",
    "                testing_vf.table.reshape(5, 5).detach().numpy()\n",
    "            )\n",
    "            plt.show()\n",
    "    \n",
    "    plot() \n",
    "    button = ipywidgets.Button(description=\"Step 100\")\n",
    "    button.on_click(lambda b: step(num_iter=100))\n",
    "    button2 = ipywidgets.Button(description=\"Step 1000\")\n",
    "    button2.on_click(lambda b: step(num_iter=1000))\n",
    "    display(output, button, button2)\n",
    "        \n",
    "\n",
    "interact(\n",
    "    q_learning,\n",
    "    gamma=ipywidgets.FloatSlider(\n",
    "        value=0.9, min=0., max=0.99, step=1e-2, continuous_update=False),\n",
    "    alpha=ipywidgets.FloatSlider(\n",
    "        value=0.5, min=0., max=2.0, step=1e-2, continuous_update=False),\n",
    "    eps=ipywidgets.FloatSlider(\n",
    "        value=0., min=0., max=1.0, step=1e-2, continuous_update=False),\n",
    "    optimistic_init=ipywidgets.Checkbox(value=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Guide:\n",
    "\n",
    "#### some explanation\n",
    "This demo shows one of the Q learning method on the grid world example.\n",
    "Note that Q learning is model free which means it learns the value function directly.\n",
    "- gamma: the discount factor of the environment.\n",
    "- alpha: the learning rate in the Q learning\n",
    "- eps: the parameter trading the exploration and exploitation. With epislon probability will pick random action and 1-epislon pick the best action.\n",
    "- optimistic init: implement the optimistic Q learning. More details could be checked the [work](https://papers.nips.cc/paper/2001/file/6f2688a5fce7d48c8d19762b88c32c3b-Paper.pdf)\n",
    "\n",
    "\n",
    "- step 100: starting in the up left corner, forward 100 steps and collect samples.\n",
    "- step 1000:  starting in the up left corner, forward 1000 steps and collect samples.\n",
    "\n",
    "- Estimated value: the estimated value function learned by interacting the world.\n",
    "- Policy: the optimal policy based on the learned MDP\n",
    "- Exploration value: given the policy we can evaluate the policy based on the epislon greedy algorithm by solving the Bellman equation by solving linear system. \n",
    "- Testing value: given the policy we can evaluate the policy by solving the Bellman equation by solving linear system.\n",
    "\n",
    "\n",
    "#### play around\n",
    "- Play with the 0 epislon and see how is the Q learning performance? Think about why it get stuck.\n",
    "- Use the optimistic initialization. Check the difference with of setting the initial value 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning with function approximation\n",
    "\n",
    "- Q Learning: approximate Q with a parametric function. \n",
    "- DQN: Approximate Q with a parametric function and use a target network to compute the delays. \n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "- DDQN: Approximate Q with a parametric function and use the target network to compute the maximum. See https://arxiv.org/pdf/1509.06461.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb892dd5b8c74b42a5d679289581588b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='env_name', options=('CartPole-v0', 'Acrobot-v1', 'MountainCar-v0')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rllib.policy import EpsGreedy, SoftMax\n",
    "from rllib.util.parameter_decay import ExponentialDecay\n",
    "\n",
    "def run(env_name, agent_name, exploration):\n",
    "    if \"CartPole\" in env_name:\n",
    "        max_steps = 200\n",
    "    else:\n",
    "        max_steps = 1000 \n",
    "        \n",
    "    environment = GymEnvironment(env_name)\n",
    "    agent = getattr(\n",
    "        importlib.import_module(\"rllib.agent\"), \n",
    "        f\"{agent_name}Agent\"\n",
    "    ).default(environment)\n",
    "    \n",
    "    if exploration == \"eps-greedy\":\n",
    "        policy = EpsGreedy(agent.algorithm.critic, ExponentialDecay(start=1.0, end=0.01, decay=500))\n",
    "    elif exploration == \"softmax\":\n",
    "        policy = SoftMax(agent.algorithm.critic, ExponentialDecay(start=1.0, end=0.01, decay=500))\n",
    "    agent.set_policy(policy)\n",
    "    try:\n",
    "        train_agent(environment=environment, agent=agent, num_episodes=40, max_steps=max_steps, render=True, plot_flag=False)\n",
    "    except KeyboardInterrupt:\n",
    "        pass \n",
    "    environment.close()\n",
    "        \n",
    "    plt.plot(agent.logger.get(\"train_return-0\"), linewidth=16)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "interact(\n",
    "    run,\n",
    "    env_name = [\"CartPole-v0\", \"Acrobot-v1\", \"MountainCar-v0\"],\n",
    "    agent_name = [\"QLearning\", \"DQN\", \"DDQN\"],\n",
    "    exploration = [\"softmax\", \"eps-greedy\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Guide:\n",
    "\n",
    "#### some explanation\n",
    "This demo shows running the model free reinforcement learning on different enviornment with different function approximation techniques.\n",
    "\n",
    "- env_name: CartPole, Acrobot, MountainCar. Note that the Acrobot and MountainCar will take longer time for convergence.\n",
    "- agent_name: QLearning, DQN, DDQN. Different function approximation techniques.  \n",
    "- alpha: the learning rate in the Q learning\n",
    "- exploration: \n",
    "    - softmax: A soft-max policy is one that has a policy given by: $\\pi(a|s) \\propto \\rho(a|s) \\exp[q(s, a) / \\tau] $,\n",
    "    where$ \\rho(a|s)$ is a prior policy, usually selected at random.\n",
    "\n",
    "\n",
    "#### play around\n",
    "- Play with the different approximation method on different environment\n",
    "- Compare the difference of softmax exploration and eps-greedy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular SARSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {},
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be58eefba3124bbaa9d247876adf22f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.9, continuous_update=False, description='gamma', max=0.99, step=0.01â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment = EasyGridWorld()\n",
    "def sarsa(gamma=0.9, alpha=0.5, eps=0., optimistic_init=False):\n",
    "    output = ipywidgets.Output()\n",
    "    global state, action \n",
    "    q_function = TabularQFunction(num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "    nn.init.ones_(q_function.nn.head.weight)\n",
    "    if optimistic_init:\n",
    "        q_function.nn.head.weight.data = 10 / (1 - gamma) * q_function.nn.head.weight.data\n",
    "        \n",
    "    state = environment.reset()\n",
    "    if np.random.rand() < eps:\n",
    "        action = np.random.choice(environment.num_actions)\n",
    "    else:\n",
    "        action = torch.argmax(q_function(torch.tensor(state).long())).item()\n",
    "\n",
    "    def step(num_iter):\n",
    "        global state, action\n",
    "        for i in range(num_iter):\n",
    "            q_val = q_function(torch.tensor(state).long(), torch.tensor(action).long())\n",
    "\n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "    \n",
    "            if np.random.rand() < eps:\n",
    "                next_action = np.random.choice(environment.num_actions)\n",
    "            else:\n",
    "                next_action = torch.argmax(q_function(torch.tensor(next_state).long())).item()\n",
    "\n",
    "\n",
    "            next_q = q_function(torch.tensor(next_state).long(), torch.tensor(next_action).long())\n",
    "            reward = torch.tensor(reward).double()\n",
    "            td = reward + gamma * next_q - q_val \n",
    "\n",
    "            q_function.set_value(state, action, q_val + alpha * td)\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "        plot()\n",
    "              \n",
    "    \n",
    "    def plot():\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            policy = extract_policy(q_function)\n",
    "            value_function = integrate_q(q_function, policy)\n",
    "            exploration_vf = linear_system_policy_evaluation(environment, EpsGreedyPolicy(policy, eps), gamma)\n",
    "            testing_vf = linear_system_policy_evaluation(environment, policy, gamma)\n",
    "\n",
    "\n",
    "            plot_all(\n",
    "                value_function.table.reshape(5, 5).detach().numpy(),\n",
    "                policy.table.argmax(0).reshape(5, 5).detach().numpy(),\n",
    "                exploration_vf.table.reshape(5, 5).detach().numpy(), \n",
    "                testing_vf.table.reshape(5, 5).detach().numpy()\n",
    "            )\n",
    "            plt.show()\n",
    "        \n",
    "    plot() \n",
    "    button = ipywidgets.Button(description=\"Step 100\")\n",
    "    button.on_click(lambda b: step(num_iter=100))\n",
    "    button2 = ipywidgets.Button(description=\"Step 1000\")\n",
    "    button2.on_click(lambda b: step(num_iter=1000))\n",
    "    display(output, button, button2)\n",
    "\n",
    "interact(\n",
    "    sarsa, \n",
    "    gamma=ipywidgets.FloatSlider(value=0.9, min=0., max=0.99, step=1e-2, continuous_update=False),\n",
    "    alpha=ipywidgets.FloatSlider(value=0.5, min=0., max=2.0, step=1e-2, continuous_update=False),\n",
    "    eps=ipywidgets.FloatSlider(value=0., min=0., max=1.0, step=1e-2, continuous_update=False),\n",
    "    optimistic_init=ipywidgets.Checkbox(value=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {}
   },
   "source": [
    "## Demo Guide:\n",
    "\n",
    "#### some explanation\n",
    "This demo runs SARSA reinforcment learning approach: SARSA algorithm is a slight variation of the popular Q-Learning algorithm. For a learning agent in any Reinforcement Learning algorithm itâ€™s policy can be of two types:- \n",
    " \n",
    "Q-Learning technique is an Off Policy technique and uses the greedy approach to learn the Q-value. SARSA technique, on the other hand, is an On Policy and uses the action performed by the current policy to learn the Q-value.\n",
    "This difference is visible in the difference of the update statements for each technique:- \n",
    "\n",
    "Q-Learning: $Q(s_{t},a_{t}) = Q(s_{t},a_{t}) + \\alpha (r_{t+1}+\\gamma max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t}))$\n",
    "\n",
    "SARSA: $Q(s_{t},a_{t}) = Q(s_{t},a_{t}) + \\alpha (r_{t+1}+\\gamma Q(s_{t+1},a_{t+1})-Q(s_{t},a_{t}))$\n",
    "\n",
    "Here, the update equation for SARSA depends on the current state, current action, reward obtained, next state and next action. This observation lead to the naming of the learning technique as SARSA stands for State Action Reward State Action which symbolizes the tuple (s, a, r, sâ€™, aâ€™).\n",
    "\n",
    "- gamma: the discount factor of the environment.\n",
    "- alpha: the learning rate in the Q learning\n",
    "- eps: the parameter trading the exploration and exploitation. With epislon probability will pick random action and 1-epislon pick the best action.\n",
    "- optimistic init: implement the optimistic Q learning. More details could be checked the [work](https://papers.nips.cc/paper/2001/file/6f2688a5fce7d48c8d19762b88c32c3b-Paper.pdf)\n",
    "\n",
    "- step 100: starting in the up left corner, forward 100 steps and collect samples.\n",
    "- step 1000:  starting in the up left corner, forward 1000 steps and collect samples.\n",
    "\n",
    "- Estimated value: the estimated value function learned by interacting the world.\n",
    "- Policy: the optimal policy based on the learned MDP\n",
    "- Exploration value: given the policy we can evaluate the policy based on the epislon greedy algorithm by solving the Bellman equation by solving linear system. \n",
    "- Testing value: given the policy we can evaluate the policy by solving the Bellman equation by solving linear system.\n",
    "\n",
    "\n",
    "#### play around\n",
    "- Similarly to the Q learning example, play around and check the difference\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
